{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "#Set Working Directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/Users/aliceroberts36/Box Sync/Dissertation (1)/Text Analysis Paper/Data/CMS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions to scrape website\n",
    "\n",
    "def scrape_prep(link, home):\n",
    "    \"\"\"This function prepares a URL for further processing.  You provide two arguments:\n",
    "        link=the link you want to scrape and prepare\n",
    "        home=for use with sub-menus.  if the link you provide is a menu link (so, '/Services' instead of a full site)\n",
    "                you put the URL where the menu originates here ('www.awebsite.com').  \n",
    "                If link is a regular site, place an empty string here.\"\"\"\n",
    "    #Prepare the url\n",
    "    olink=link\n",
    "    nhome=home\n",
    "    \n",
    "    match = re.search(r\"http\", home)\n",
    "    if match==None:\n",
    "        nhome=(\"http://\"+home)  \n",
    "    \n",
    "    www = re.search(r\"www.\", link)\n",
    "    if www==None:\n",
    "        olink = urljoin(nhome,link)\n",
    "        \n",
    "    match = re.search(r\"http\", olink)\n",
    "    if match==None:\n",
    "        olink=(\"http://\"+link)\n",
    "    #print(olink) #turn this on to troubleshoot\n",
    "    #Import the text\n",
    "    try:\n",
    "        r = requests.get(olink)\n",
    "    except:\n",
    "        print(olink, \"Error!\")\n",
    "        return \"\"\n",
    "        \n",
    "    html_doc = r.text\n",
    "    \n",
    "    #Turn it into soup\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')\n",
    "    psoup = BeautifulSoup.prettify(soup)\n",
    "    \n",
    "    #Return the edited document\n",
    "    return soup\n",
    "\n",
    "def scrape_main(soup):\n",
    "    \"\"\"This returns all of the text in the provided soup, without cleaning or editing.  Will include menu items.\"\"\"  \n",
    "    alltxt = soup.get_text()\n",
    "    return alltxt\n",
    "\n",
    "def scrape_menu(soup, types):\n",
    "    \"\"\"This returns all menu items.  Depending on the website, it may include sub-menu items.\n",
    "    soup=soup of website you want\n",
    "    types= what type of return you want:\n",
    "        'link' returns menu links\n",
    "        'text' returns the menu names\n",
    "        'dict' returns a dictionary where the menu names are keys and the URLs are values\n",
    "    Can be told to grab the text, links, or a dictionary that contains both.\"\"\"\n",
    "    #Initialize lists\n",
    "    menu = []\n",
    "    menlk = []\n",
    "    #Loop over the items and fill lists with names and URLs.\n",
    "    for tag in soup.find_all(\"li\"):\n",
    "        #Get text\n",
    "        for a in tag.find_all(\"a\"):\n",
    "            tagname=[]\n",
    "            for name in a.text.split('\\n'):\n",
    "                if len(name) > 0:\n",
    "                    menu.append(name.strip())\n",
    "            taglen=len(tagname)\n",
    "            #print(\"Tags:\", taglen)\n",
    "            #fulltag = \" \".join(tagname)\n",
    "            #menu.append(tagname)\n",
    "            #Get links\n",
    "            menu_links=tag.find_all(\"a\")\n",
    "            lks = []\n",
    "            for link in menu_links:\n",
    "                lks.append(link.get(\"href\"))\n",
    "            #print(\"Links: \",len(lks))\n",
    "            if lks!=[]:\n",
    "                menlk.append(lks[0])\n",
    "            else:\n",
    "                menlk.append(\"\")\n",
    "    \n",
    "    #Return the appropriate information to user, based on string \"types\"\n",
    "    if types==\"link\":\n",
    "        return menlk\n",
    "    if types==\"text\":\n",
    "        return menu\n",
    "    if types==\"dict\":\n",
    "        key=menu\n",
    "        values=menlk\n",
    "        dictionary = dict(zip(key, values))\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "    \n",
    "def skinny_scrape(soup):\n",
    "    \"\"\"This scrapes all tagged 'paragraphs' from the website.  May miss some information.\n",
    "    Provide soup (which can be generated by scrape_prep)\"\"\" \n",
    "    parags=[]\n",
    "    for tag in soup.find_all(\"p\"):\n",
    "        for name in tag.text.split('\\n'):\n",
    "            if len(name) > 0:\n",
    "                parags.append(name.strip())\n",
    "    return parags\n",
    "\n",
    "\n",
    "def scrape_links(soup):\n",
    "    \"\"\"Give this function the soup and it will return all links from the site as a list\"\"\"\n",
    "    links=[]\n",
    "    for lk in soup.find_all(\"a\"):\n",
    "        link = lk.find('href')\n",
    "        links.append(link)\n",
    "    return links\n",
    "        \n",
    "    \n",
    "\n",
    "def scrape_select(dic, term, types, home):\n",
    "    \"\"\"This function pulls from the menu, opening the link associated with a term and getting requested contents:\n",
    "        menu links and names, paragraph content\n",
    "        It accepts four arguments:\n",
    "        dic=dictionary which should include tab names and links (note that scrape_menu can provide this)\n",
    "        term=the term you want to find in the menu\n",
    "        types=the type of return you want, which accepts:\n",
    "            tabname (you want the name of the tab that contained the search term)\n",
    "            parags (you want the plain text from the page)\n",
    "            lks (you want the links)\n",
    "        home=the home URL for the site (as string)\n",
    "            \"\"\"\n",
    "    soup=\"\"\n",
    "    tabname=\"\"\n",
    "    for each in dic:\n",
    "        match = re.search(term, each)\n",
    "        if match!=None:\n",
    "            if types==\"tabname\":\n",
    "                tabname=each\n",
    "                return tabname\n",
    "            else:\n",
    "                soup = scrape_prep(dic[each], home)\n",
    "    if soup==\"\":\n",
    "        #print(\"No matches\")\n",
    "        return \"\"\n",
    "    if types==\"parags\":\n",
    "        para=skinny_scrape(soup)\n",
    "        return para\n",
    "    else:\n",
    "        lks=scrape_menu(soup, types)\n",
    "        return lks\n",
    "                   \n",
    "\n",
    "def scrape_find(dic: dict, term: str, types:str):\n",
    "    \"\"\"This function identifies all matching cases in provided menu, and returns a list.\n",
    "        Arguments include:\n",
    "            -dic (your dictionary)\n",
    "            -term (what you want to search for)\n",
    "            -types (what type of output you want)\n",
    "                -types can be either 'text' or 'link'\"\"\"\n",
    "    matches=[]\n",
    "    for each in dic:\n",
    "        match=re.search(term, each)\n",
    "        if match!=None:\n",
    "            if types==\"text\":\n",
    "                matches.append(each)\n",
    "            if types==\"link\":\n",
    "                matches.append(dic[each])\n",
    "                \n",
    "    return matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataset\n",
    "##CSV file with URLs and other hospital data, read in as a pandas dataframe\n",
    "georgia = pd.read_csv(\"appended2.csv\")\n",
    "\n",
    "#Create a list object of just the URLs (in my dataset, the urls are under a column named \"url\".\n",
    "urls = georgia.url\n",
    "\n",
    "\n",
    "#Create new variables to populate (used in next section)\n",
    "georgia['soup']=np.nan\n",
    "georgia['menu']= np.nan\n",
    "georgia['hometext']=np.nan\n",
    "georgia['bartext']=np.nan\n",
    "georgia['bartab']=np.nan\n",
    "georgia['davinci']=np.nan\n",
    "georgia['datab']=np.nan\n",
    "georgia['misstab']=np.nan\n",
    "georgia['misstext']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/aliceroberts36/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error!\n",
      "Error!\n"
     ]
    }
   ],
   "source": [
    "#Scrape Content from all URLs\n",
    "##This iterates through any list of URLs--I compiled my list through www.ahd.com using a free education account.\n",
    "###If you want to publish, AHD may provide you free access to their data, even without the EDU account.\n",
    "i=0 #initialize count\n",
    "for url in urls:\n",
    "    #print(url) #Turn on to troubleshoot\n",
    "    if url is not np.nan:\n",
    "        #Prep the url for scraping\n",
    "        urlsoup=scrape_prep(url, \"\") ##will return a blank string if the URL is broken and print \"Error!\"\n",
    "        if urlsoup != \"\":\n",
    "            #Get the menu\n",
    "            menu = scrape_menu(urlsoup, \"dict\")\n",
    "            mentxt = [*menu] #changes menu into a list of keys\n",
    "            georgia.menu[i] = \"; \".join(mentxt)\n",
    "\n",
    "            #Get the text from home page\n",
    "            text = skinny_scrape(urlsoup)\n",
    "            georgia.hometext[i] = \"; \".join(text)\n",
    "\n",
    "            #Get the info for bariatric surgery (if it exists)\n",
    "            barterms = [\"Weight Loss\", \"Weight-Loss\", \"bariatric\", \"weightloss\", \"weight loss\", \"weight-loss\"]\n",
    "            #Get tab names\n",
    "            bartab=\"\"\n",
    "            for term in barterms:\n",
    "                if bartab==\"\":\n",
    "                    bartab = scrape_select(menu, term, \"tabname\", url)\n",
    "                    if bartab != \"\":\n",
    "                        bartext= scrape_select(menu, term, \"parags\", url)\n",
    "\n",
    "            if bartab!=\"\":\n",
    "                georgia.bartab[i] = bartab\n",
    "                #Get text information\n",
    "                georgia.bartext[i] =\"; \".join(bartext)\n",
    "\n",
    "            #Get the Da Vinci info\n",
    "            daterms = ['Da Vinci', \"Robotic Surgery\", \"Robotic\"]\n",
    "            datab=\"\"\n",
    "            for term in daterms:\n",
    "                if datab==\"\":\n",
    "                    datab=scrape_select(menu, term, \"tabname\", url)\n",
    "                    if datab!=\"\":\n",
    "                        datext=scrape_select(menu, term, \"parags\", url)\n",
    "\n",
    "            if datab!=\"\":\n",
    "                georgia.datab[i]=datab\n",
    "                georgia.davinci[i]=\"; \".join(datext)\n",
    "\n",
    "        #Get the mission statements\n",
    "            missterms = ['Mission','Purpose']\n",
    "            termtab=\"\"\n",
    "            for term in missterms:\n",
    "                if termtab==\"\":\n",
    "                    termtab=scrape_select(menu, term, \"tabname\", url)\n",
    "                    if termtab!=\"\":\n",
    "                        misstext=scrape_select(menu, term, \"parags\", url)\n",
    "            if termtab!=\"\":\n",
    "                georgia.misstab[i]=termtab\n",
    "                georgia.misstext[i]=\"; \".join(misstext)\n",
    "\n",
    "        \n",
    "    i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping is imperfect.  The following tests a few findings. Everything is currently commented out, but the code is\n",
    "#working.\n",
    "\n",
    "##View new dataset\n",
    "#georgia\n",
    "\n",
    "\n",
    "#### Test other features (some tests are commented out)\n",
    "#type(georgia.menu[0])\n",
    "#georgia\n",
    "\n",
    "#for i in range(0,len(georgia)):\n",
    "    #print(georgia.bartext[i])\n",
    "\n",
    "#soup = scrape_prep(georgia.url[4], \"\")\n",
    "#dic = scrape_menu(soup, \"dict\")\n",
    "#print(dic)\n",
    "#tabname = scrape_select(dic, \"Bariatric\", \"tabname\", georgia.url[4])\n",
    "#print(tabname)\n",
    "#print(dic[\"Bariatrics (Weight Loss Surgery)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export newly created dataset to CSV\n",
    "georgia.to_csv(\"georgiatxt2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
